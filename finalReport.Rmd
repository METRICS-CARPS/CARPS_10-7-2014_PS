---
title: "CARPS Reproducibility Report"
output:
  html_document:
    toc: true
    toc_float: true
---

```{r}
articleID <- "10-7-2014_PS" # insert the article ID code here e.g., "10-3-2015_PS"
reportType <- 'pilot'
pilotNames <- "Jaclyn Schwartz, Drew Tysen Dauer, Erik Santoro" # insert the pilot's name here e.g., "Tom Hardwicke". If there are multiple pilots enter both names in a character string e.g., "Tom Hardwicke, Bob Dylan"
copilotNames <- "" # insert the co-pilot's name here e.g., "Michael Frank". If there are multiple co-pilots enter both names in a character string e.g., "Tom Hardwicke, Bob Dylan"
pilotTTC <- 420 # insert the pilot's estimated time to complete (in minutes, fine to approximate) e.g., 120
copilotTTC <- NA # insert the co- pilot's estimated time to complete (in minutes, fine to approximate) e.g., 120
pilotStartDate <- as.Date("10/26/2017", format = "%m/%d/%y") # insert the pilot's start date in US format e.g., as.Date("01/25/18", format = "%m/%d/%y")
copilotStartDate <- as.Date("06/20/2018", format = "%m/%d/%y") # insert the co-pilot's start date in US format e.g., as.Date("01/25/18", format = "%m/%d/%y")
completionDate <- as.Date("", format = "%m/%d/%y") # copilot insert the date of final report completion (after any necessary rounds of author assistance) in US format e.g., as.Date("01/25/18", format = "%m/%d/%y")
```

-------

# Methods summary: 
[The point of Gilead & Liberman's (2014) "Study 1" is to test if activating "caregiving motivation" would strengthen a bias against out-groups. The authors recruited 300 participants (all Jewish mothers) to participate in an online study. `N = 300` was deterimined with `75 participants per condition`. There were `4 condtions`. However, `14 were excluded`, leaving `N = 286`.
The 4 conditions were based on 2 manipulations: *caregiving-salience manipulation* (with 2 groups: caregiving-salience group and no-caregiving-salience group) and *out-group threat manipulation* (with 2 groups: out-group-threat condition and  natural-threat condition). Condition assignment was random and created by crossing the two caregiving-salience conditions with the two out-group-threat conditions. The DV of out-group bias was operationalized with an out-group bias questionnaire. The authors normalized the out-group `bias scores`, and then performed a `factorial ANOVA` with `caregiving` (caregiving salience vs. no caregiving salience) and `threat` (out-group threat vs. natural threat) as IVs, and `out-group bias` as the DV." After finding an interaction, simple effects analyses were performed.]

------

# Target outcomes: 
[The results showed a significant interaction between caregiving and threat, F(1, 282) = 4.220, p = .041, ηp2 = .014 (Fig. 1). Simple-effects analysis revealed that in the out-group-threat condition, participants who were primed for caregiving salience showed significantly higher levels of out-group bias (M = 0.851, SD = 0.528) than did the no-caregiving-salience group (M = 0.654, SD = 0.488), F(1, 282), p = .027, Cohen’s d = 0.387. In the natural-threat condition, there was no difference between participants in the caregiving-salience (M = 0.703, SD = 0.594) and no-caregiving-salience (M =
0.773, SD = 0.583) groups, F < 1. No further effects attained significance.]  

------

[The chunk below sets up some formatting options for the R Markdown document]

```{r global_options, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE)
```

## Step 1: Load packages

[Some useful packages are being loaded below. You can add any additional ones you might need too.]

```{r}
library(tidyverse) # for data munging
library(knitr) # for kable table formating
library(haven) # import and export 'SPSS', 'Stata' and 'SAS' Files
library(readxl) # import excel files
library(CARPSreports) # custom report functions
library(ggplot2)
library(ggthemes)
```

```{r}
# Prepare report object. This will be updated automatically by the reproCheck function each time values are compared.
reportObject <- data.frame(dummyRow = TRUE, reportedValue = NA, obtainedValue = NA, valueType = NA, percentageError = NA, comparisonOutcome = NA, eyeballCheck = NA)
```

## Step 2: Load data

```{r}
caregiving_salience <- read_excel("data/caregiving_salience_paper_raw_data.xlsx", sheet = 1) %>%
  mutate(story=factor(story, levels=c("control", "caregiving")),  # for reproducing plots need this order
         tnaythreat=factor(tnaythreat, levels=c("threat", "control")))
head(caregiving_salience)
str(caregiving_salience)
```

## Step 3: Tidy data

```{r}
# the data was already in long format and the bias scores were already normalized
```

# Preliminaries

Check exclusions
```{r exclusision}
cat('Exclusions not included in (Study 1 methods.)?:', nrow(caregiving_salience) == 300 - 14)
```



## Step 4: Run analysis

# Descriptive statistics

```{r summary-stats}
df_descriptives <- caregiving_salience %>%
  group_by(story, tnaythreat) %>%
  summarise(mean=round(mean(bias), 3),
            sd=round(sd(bias), 3))

kable(df_descriptives, caption="Descriptive stats")
```

```{r cache-report-check-descriptives}
res_descriptives <- data.frame()
descriptives_codes <- list(
  # outgroup threat
  c('0.851','threat', 'caregiving', 'mean'),
  c('0.528','threat', 'caregiving', 'sd'),
  c('0.654','threat', 'control', 'mean'),
  c('0.488','threat', 'control', 'sd'),
  # natural threat
  c('0.703','control', 'caregiving', 'mean'),
  c('0.594','control', 'caregiving', 'sd'),
  c('0.773','control', 'control', 'mean'),
  c('0.583','control', 'control', 'sd'))
  
  
for (i in 1:length(descriptives_codes)) {
  reported_stat <- descriptives_codes[[i]][1]
  tnaythreat_ <- descriptives_codes[[i]][2]
  story_ <- descriptives_codes[[i]][3]
  stat_type <- descriptives_codes[[i]][4]
  if (stat_type == 'mean') {
    res_descriptives <- rbind(res_descriptives, 
                            data.frame("reportedValue"=reported_stat, 
                                       "obtainedValue"=df_descriptives %>% 
                                         filter(tnaythreat==tnaythreat_, story==story_) %>% 
                                         ungroup %>% 
                                         select(mean) %>%
                                         as.numeric(), 
                                       "valueType"=stat_type,
                                       "eyeballCheck"=NA))
  } else {
    res_descriptives <- rbind(res_descriptives, 
                            data.frame("reportedValue"=reported_stat, 
                                       "obtainedValue"=df_descriptives %>% 
                                         filter(tnaythreat==tnaythreat_, story==story_) %>% 
                                         ungroup %>% 
                                         select(sd) %>%
                                         as.numeric(), 
                                       "valueType"=stat_type,
                                       "eyeballCheck"=NA))
    
  }
}
```


## Reprocuing fig1
```{r reproduce-fig1}
caregiving_salience %>%
  group_by(story, tnaythreat) %>%
  summarise(mean_=mean(bias),
            sd_=sd(bias),
            n_=n(),
            se_= sd_ / sqrt(n_)) %>%
  ggplot(aes(x=tnaythreat, y=mean_, fill=story)) +
    geom_bar(stat='identity', position='dodge') +
    geom_errorbar(aes(ymin=mean_-se_, ymax=mean_+se_), position=position_dodge(.95), width=0.25) +
    labs(title="Outgroup Bias ratings by caregiving salience and threat condition", 
         y="normalized out-group bias", 
         x="threat condition ") + 
    ylim(0, 1) +
    theme_classic() +
    theme(plot.title = element_text(hjust = 0.5))
```

![](img/fig1.png)

### Inferential statistics

The outcome I am aiming to reproduce is the significant interaction between caregiving and threat.
```{r}
library(broom)
aov_int <- aov(bias ~ story * tnaythreat, data=caregiving_salience)
aov_int_sum <- summary(aov_int) 
knitr::kable(broom::tidy(aov_int)) # F(1,282) = 4.22, p = .041
pval <- aov_int_sum[[1]][[3,"Pr(>F)"]] # 0.0408824
pval_r <- round(pval, digits = 3) # 0.041
num_df <- aov_int_sum[[1]][[1]][[1]] # 1
den_df <- aov_int_sum[[1]][[1]][[4]] # 282
Fstat <- aov_int_sum[[1]][[4]][[3]] # 4.21955
Fstat_r <- round(Fstat, digits = 3) # 4.22
 
library(lsr)
aov_int_eta <- etaSquared(aov_int , type = 2, anova = TRUE)
aov_int_eta.sq.part <- aov_int_eta[[3,2]] # 0.0147423549 
aov_int_eta.sq.part_r <- round(aov_int_eta.sq.part, digits = 3) # 0.015

# Comparing p value
reportObject <- reproCheck(reportedValue = ".041", obtainedValue = pval_r, valueType = 'p') # MATCH!

# Comparing numerator DF
reportObject <- reproCheck(reportedValue = "1", obtainedValue = num_df, valueType = 'df') # MATCH!

# Comparing denominator DF
reportObject <- reproCheck(reportedValue = "282", obtainedValue = den_df, valueType = 'df') # MATCH!

# Comparing F stat
reportObject <- reproCheck(reportedValue = "4.220", obtainedValue = Fstat_r, valueType = 'F') # MATCH!

# Comparing partial eta squared
reportObject <- reproCheck(reportedValue = ".014", obtainedValue = aov_int_eta.sq.part_r, valueType = 'pes') # MINOR NUMERICAL ERROR
```
I performed an ANOVA as was reported in the paper and indeed, obtained the same conclusion. There is a significant interaction between caregiving and threat,  *F*(`r num_df`,`r den_df`) = `r Fstat_r`, *p* = `r pval_r`, $\eta_{p}^{2}$ = `r aov_int_eta.sq.part_r`. The reported p value, DF, and F statistic were matches. However, there was a minor numerical error in the partial eta squared, by 7.14%.

Following this interaction, the paper reports post-hoc simple-effects analyses. Here, I am aiming to reproduce that in the `out-group-threat` condition, participants who were primed for *caregiving-salience* showed significantly higher levels of out-group bias than did the *no-caregiving-salience* group. I will also aim to reproduce that in the `natural-threat` condition, there was no difference between participants in the *caregiving-salience* and *no-caregiving-salience* groups.  
```{r}
# Performing post-hoc tests from the ANOVA
MultCompar <- knitr::kable(broom::tidy(TukeyHSD(aov_int)))
MultCompar
# compare Values with Tukey HSD
reportObject <- reproCheck(reportedValue = ".027", obtainedValue = 0.1200284, valueType = 'p') 
```
The TukeyHSD test at the beginning of this code chunk gives me the tests for multiple comparisons from the ANOVA. According to my google searches, TukeyHSD of the ANOVA should be fine to find out which group means are different. However, I found very different values than what was reported. From emailing back and forth with Tom, we figured I'd try filtering the data. I therefore am *not including this decsion error and major numerical error in my carpsReport* since this may have not been the appropriate test to perform. I am next going to subset the `tnaythreat` variable into "threat" and "control", as was recommended, in order to continue my aim in reproducing the simple effects from the interaction in the ANOVA.

This chunk of code subsets the original dataframe `caregiving_salience` into a dataframe for the `out-group-threat` condition and `natural-threat` condition in order to look at difference in `story` manipulation under each condition of threat. 
```{r}
# Subsetting dataframe by the tnaythreat condition to look at difference in "story" under outgroup-threat and under natural-threat
dataThreat <- subset(caregiving_salience, tnaythreat == "threat")
dataCTL <- subset(caregiving_salience, tnaythreat == "control") 
```

I then perform simple effects analysis of `story` under the `out-group-threat` condition. I am performing F tests for these simple effects since that is what the paper reports. I aim aiming to reproduce the report that in the `out-group-threat` condition, participants who were primed for *caregiving salience* showed significantly higher levels of out-group bias than did the *no-caregiving-salience* group.
```{r}
# Simple Effect of story manipulation under the out-group-threat condition
StoryCGvsCTL_outgrpthreat <-aov(bias ~ story, data = dataThreat)
StoryCGvsCTL_outgrpthreat_sum <- summary(StoryCGvsCTL_outgrpthreat) 
knitr::kable(broom::tidy(StoryCGvsCTL_outgrpthreat)) 
SEpval <- StoryCGvsCTL_outgrpthreat_sum[[1]][[1,"Pr(>F)"]] # 0.01827411
SEpval_r <- round(SEpval, digits = 3) # 0.018
SEnum_df <- StoryCGvsCTL_outgrpthreat_sum[[1]][[1]][[1]] # 1
SEden_df <- StoryCGvsCTL_outgrpthreat_sum[[1]][[1]][[2]] # 151
SEcohensD <- cohensD(x = bias~story, data = dataThreat) # 0.3871863
SEcohensD_r <- round(SEcohensD, digits = 3) # 0.387

# Means and SDs of story manipulation condtion under out-group-threat condition
attach(dataThreat)
mean_CGvsCTL_OGthreat <- tapply(bias, story, mean)
mean_CG_OGthreat <- mean_CGvsCTL_OGthreat[[1]][[1]] # 0.8519226
mean_CG_OGthreat_r <- round(mean_CG_OGthreat, digits = 3) # 0.852
mean_CTL_OGthreat <- mean_CGvsCTL_OGthreat[[2]][[1]] # 0.6542341
mean_CTL_OGthreat_r <- round(mean_CTL_OGthreat, digits = 3) # 0.654
SD_CGvsCTL_OGthreat <- tapply(bias, story, sd)
SD_CG_OGthreat <- SD_CGvsCTL_OGthreat[[1]][[1]] # 0.5280573
SD_CG_OGthreat_r <- round(SD_CG_OGthreat, digits = 3) # 0.528
SD_CTL_OGthreat <- SD_CGvsCTL_OGthreat[[2]][[1]] # 0.4889919
SD_CTL_OGthreat_r <- round(SD_CTL_OGthreat, digits = 3) # 0.489

# The following comparisons (compareValues) are for the simple effects reporting caregiving vs. non-caregiving under out-group-threat condition

# Comparing p value
reportObject <- reproCheck(reportedValue = ".027", obtainedValue = SEpval_r, valueType = 'p') # MAJOR NUMERICAL ERROR.

# Comparing numerator DF
reportObject <- reproCheck(reportedValue = "1", obtainedValue = SEnum_df, valueType = 'df') # MATCH!

# Comparing denominator DF
reportObject <- reproCheck(reportedValue = "282", obtainedValue = SEden_df, valueType = 'df') # MAJOR NUMERICAL ERROR

# Comparing effect size 
reportObject <- reproCheck(reportedValue =  "0.387", obtainedValue = SEcohensD_r, valueType = 'd') # MATCH!

# Comparing reported mean of caregiving 
reportObject <- reproCheck(reportedValue = "0.851", obtainedValue = mean_CG_OGthreat_r, valueType = 'mean') # MINOR NUMERICAL ERROR

# Comparing reported SD of caregiving
reportObject <- reproCheck(reportedValue = "0.528", obtainedValue = SD_CG_OGthreat_r, valueType = 'sd') # MATCH!

# Comparing reported mean of non-caregiving 
reportObject <- reproCheck(reportedValue = "0.654", obtainedValue = mean_CTL_OGthreat_r, valueType = 'mean') # MATCH! 

# Comparing reported SD of non-caregiving
reportObject <- reproCheck(reportedValue = "0.488", obtainedValue = SD_CTL_OGthreat_r, valueType = 'sd') # MINOR NUMERICAL ERROR
```
The values I obtained are very close to what was reported, but there do seem to be some errors, possibly due to rounding? I indeed obtained that under the `out-group-threat` condition, there were significnatly higher levels of out-group bias for those primed for *caregiving-salience* (M = `r mean_CG_OGthreat_r`, SD = `r SD_CG_OGthreat_r`) compared to *no-caregiving-salience* (M = `r mean_CTL_OGthreat_r`, SD = `r SD_CTL_OGthreat_r`), *F*(`r SEnum_df`, `r SEden_df`), *p* = `r SEpval_r`, Cohen's d = `r SEcohensD_r`. However, there were still numerical errors. There was a major numerical error by 33.33% in *p* values. For the degrees of freedom, there was a match in the numerator, but a major numerical error in the denominator by 46.45%. This is very likely due to my subsetting the data. I'm not sure how the authors were able to conduct simple effects ANOVAS on the full dataset, but subsetting the data was the only way I was able to perform these simple effect F tests. The effect size reported and obtained was a match. There was a minor numerical error in the mean of out-group bias in the *caregiving-salience* group by .12%, but a match in the SD of out-group bias in this group. There was a match in mean of out-group bias in the *non-caregiving salience* group, but a minor numerical error in the SD by .2% of out-group bias in this group.

I then perform simple effects analysis (ANOVA) of `story` under the `natural-threat` condition.
```{r}
# Simple Effect of story manipulation under the natural-threat condition
StoryCGvsCTL_naturalthreat <-aov(bias ~ story, data = dataCTL)
StoryCGvsCTL_naturalthreat_sum <- summary(StoryCGvsCTL_naturalthreat) # F value is 0.469 (< 1)
knitr::kable(broom::tidy(StoryCGvsCTL_naturalthreat)) 

# Means and SDs of story manipulation condtion under out-group-threat condition
attach(dataCTL)
mean_CGvsCTL_natthreat <- tapply(bias, story, mean)
mean_CG_natthreat <- mean_CGvsCTL_natthreat[[1]][[1]] # 0.7035284
mean_CG_natthreat_r <- round(mean_CG_natthreat, digits = 3) # 0.704
mean_CTL_natthreat <- mean_CGvsCTL_natthreat[[2]][[1]] # 0.7735317
mean_CTL_natthreat_r <- round(mean_CTL_natthreat, digits = 3) # 0.774
SD_CGsvsCTL_natthreat <- tapply(bias, story, sd)
SD_CG_natthreat <- SD_CGsvsCTL_natthreat[[1]][[1]] # 0.5943702
SD_CG_natthreat_r <- round(SD_CG_natthreat, digits = 3) # 0.594
SD_CTL_natthreat <- SD_CGsvsCTL_natthreat[[2]][[1]] # 0.5834817
SD_CTL_natthreat_r <- round(SD_CTL_natthreat, digits = 3) # 0.583

# The following comparisons are for the simple effects reporting caregiving vs. non-caregiving under natural-threat condition

# Comparing reported mean of caregiving 
reportObject <- reproCheck(reportedValue = "0.703", obtainedValue = mean_CG_natthreat_r, valueType = 'mean') # MINOR NUMERICAL ERROR

# Comparing reported SD of caregiving
reportObject <- reproCheck(reportedValue = "0.594", obtainedValue = SD_CG_natthreat_r, valueType = 'sd') # MATCH!

# Comparing reported mean of non-caregiving 
reportObject <- reproCheck(reportedValue = "0.773", obtainedValue = mean_CTL_natthreat_r, valueType = 'mean') # MINOR NUMERICAL ERROR

# Comparing reported SD of non-caregiving
reportObject <- reproCheck(reportedValue = "0.583", obtainedValue = SD_CTL_natthreat_r, valueType = 'sd') # MATCH!
```
I obtained similar results to what the paper reports for the `natural-threat` condition. In this condition, there was no difference between those in the *caregiving-salience* group (M = `r mean_CG_natthreat_r`, SD = `r SD_CG_natthreat_r`) and the *no-caregiving-salience* group (M = `r mean_CTL_natthreat_r`, SD = `r SD_CTL_natthreat_r`). The paper reports the F value < 1, which is what I obtained (F = .495); however, I couldn't perform the compareValues function on this becasue I didn't have an exact number for the reported F value. Technically, this would be an INSUFFICIENT INFORMATION ERROR; however, .495 is still less than 1, which is what was reported, so I would consider this a MATCH. There is a minor numerical error for the reported and obtained mean of out-group bias for the *caregiving-salience* group under `natural-threat` condition by .14%, but there is a match in SD of out-group bias for those primed with caregiving under this condition. There is a minor numerical error for reported and obtained mean of out-group bias for the *non-caregiving* group under `natural-threat` by .13%, but there is a match in SD of out-group bias for those in the *non-caregiving* group under this condition.  

I am next subsetting the dataframe by the story condition now to look at simple effects of  `threat` under `caregiving` and under `non-caregiving` conditions to reproduce the authors' claim that there were no other effects.
```{r}
dataCG <- subset(caregiving_salience, story == "caregiving")
dataCG_CTL <- subset(caregiving_salience, story == "control") 
# Simple Effect of threat given the caregiving manipulation
Threat_givenCG <- aov(bias ~ tnaythreat, data = dataCG)
Threat_givenCG_sum <- summary(Threat_givenCG)
knitr::kable(broom::tidy(Threat_givenCG)) # F(1,150), p = 0.1053698
```

The simple effects analyses do not show any difference of out-group bias between `out-group-threat` and `natural-threat` conditions when primed under `caregiving` and `non-caregiving` manipulations. Therefore, as reported, no other effects were significant.

## Step 5: Conclusion

I performed an ANOVA as was reported in the paper and obtained very similar results for the main interaction reported. There is a significant interaction between caregiving and threat. However, there was a minor numerical error in the eta partial squared value. Following the interaction, I conducted a series of simple effects analysis with F tests, since these post-hoc F tests were reported in the paper. In order to do this, I subsetted the dataframe to perform the appropriate comparisons. For the simple effects under the `out-group-threat` condition, I reproduced the significnatly higher levels of out-group bias for participants primed for *caregiving salience* (M = `r mean_CG_OGthreat_r`, SD = `r SD_CG_OGthreat_r`) compared to those with *no-caregiving-salience* (M = `r mean_CTL_OGthreat_r`, SD = `r SD_CTL_OGthreat_r`), *F*(`r SEnum_df`, `r SEden_df`), *p* = `r SEpval_r`, Cohen's d = `r SEcohensD_r`. However, there  was a MAJOR NUMERICAL ERROR by 33.33% in *p* values and a MAJOR NUMERICAL ERROR in the denominator DF by 46.45%. This is very likely due to my subsetting the data. I'm not sure how the authors were able to conduct simple effects ANOVAS on the full dataset, but subsetting the data was the only way I was able to perform these simple effect F tests. There was also a MINOR NUMERICAL ERROR in the mean of out-group bias in the *caregiving-salience* group by .12% and a MINOR NUMERICAL ERROR in the SD by .2% of out-group bias in the *no-caregiving-salience* group. All other statistics for this comparison under the `out-group-threat` condition were MATCHES. For the simple effects under the `natural-threat` condition, I reproduced that there was no difference in out-group bias between those in the *caregiving-salience* group and *no-caregiving-salience* group. The paper reports the F value < 1, which is what I obtained (F = .495); however, I couldn't perform the compareValues function on this becasue I didn't have an exact number for the reported F value. Therefore, this is technically an INSUFFICIENT INFORMATION ERROR; however, I will report this as a MATCH because I did obtain an F value within the limit of what was reported. There was also a MINOR NUMERICAL ERROR for the reported and obtained mean of out-group bias for the *caregiving-salience* group under `natural threat` by .14%, and a MINOR NUMERICAL ERROR for reported and obtained mean of out-group bias for the *non-caregiving-salience* group under `natural threat` condition by .13%. All other statistics for this comparison under the `natural-threat` condition were MATCHES. The results also report that "No further effects attained significance." I therefore, performed simple effects ANOVAs for the *threat* manipulation under `caregiving-salience` and `non-caregiving-salience.` The simple effects analyses do not show any difference of out-group bias between `out-group-threat` and `natural-threat` conditions when primed under `caregiving-salience` and `non-caregiving-saliencce` manipulations. Therefore, as reported, no other effects were significant.  

Even with these numerical errors, the conclusions I obtain are consistent with what is reported in the paper. I believe these numerical errors occured from subsetting the dataframe in order to perform the simple effect ANOVAS.

```{r}
Author_Assistance = FALSE # was author assistance provided? (if so, enter TRUE)

Insufficient_Information_Errors <- 0 # how many discrete insufficient information issues did you encounter?

# Assess the causal locus (discrete reproducibility issues) of any reproducibility errors. Note that there doesn't necessarily have to be a one-to-one correspondance between discrete reproducibility issues and reproducibility errors. For example, it could be that the original article neglects to mention that a Greenhouse-Geisser correct was applied to ANOVA outcomes. This might result in multiple reproducibility errors, but there is a single causal locus (discrete reproducibility issue).

locus_typo <- NA # how many discrete issues did you encounter that related to typographical errors?
locus_specification <- NA # how many discrete issues did you encounter that related to incomplete, incorrect, or unclear specification of the original analyses?
locus_analysis <- NA # how many discrete issues did you encounter that related to errors in the authors' original analyses?
locus_data <- NA # how many discrete issues did you encounter that related to errors in the data files shared by the authors?
locus_unidentified <- NA # how many discrete issues were there for which you could not identify the cause

Affects_Conclusion <- NA # Do any reproducibility issues encounter appear to affect the conclusions made in the original article? This is a subjective judgement, but you should taking into account multiple factors, such as the presence/absence of decision errors, the number of target outcomes that could not be reproduced, the type of outcomes that could or could not be reproduced, the difference in magnitude of effect sizes, and the predictions of the specific hypothesis under scrutiny.
```

```{r}
reportObject <- reportObject %>%
  filter(dummyRow == FALSE) %>% # remove the dummy row
  select(-dummyRow) %>% # remove dummy row designation
  mutate(articleID = articleID) %>% # add the articleID 
  select(articleID, everything()) # make articleID first column

# decide on final outcome
if(any(reportObject$comparisonOutcome != "MATCH") | Insufficient_Information_Errors > 0){
  finalOutcome <- "Failure without author assistance"
  if(Author_Assistance == T){
    finalOutcome <- "Failure despite author assistance"
  }
}else{
  finalOutcome <- "Success without author assistance"
  if(Author_Assistance == T){
    finalOutcome <- "Success with author assistance"
  }
}

# collate report extra details
reportExtras <- data.frame(articleID, pilotNames, copilotNames, pilotTTC, copilotTTC, pilotStartDate, copilotStartDate, completionDate, Author_Assistance, finalOutcome, Insufficient_Information_Errors, locus_typo, locus_specification, locus_analysis, locus_data, locus_unidentified)

# save report objects
if(reportType == "pilot"){
  write_csv(reportObject, "pilotReportDetailed.csv")
  write_csv(reportExtras, "pilotReportExtras.csv")
}

if(reportType == "final"){
  write_csv(reportObject, "finalReportDetailed.csv")
  write_csv(reportExtras, "finalReportExtras.csv")
}
```

# Session information

```{r session_info, include=TRUE, echo=TRUE, results='markup'}
devtools::session_info()
```

